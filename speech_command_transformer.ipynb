{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\stt_env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.9380\n",
      "Epoch 2/10, Loss: 1.2769\n",
      "Epoch 3/10, Loss: 1.0876\n",
      "Epoch 4/10, Loss: 0.9995\n",
      "Epoch 5/10, Loss: 0.9530\n",
      "Epoch 6/10, Loss: 0.8923\n",
      "Epoch 7/10, Loss: 0.8470\n",
      "Epoch 8/10, Loss: 0.8180\n",
      "Epoch 9/10, Loss: 0.7867\n",
      "Epoch 10/10, Loss: 0.7556\n",
      "Test Accuracy: 80.36%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCHAUDIO_BACKEND\"] = \"sox_io\"  # sox_io 백엔드를 사용하도록 지정\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 데이터셋 클래스 정의\n",
    "# SPEECHCOMMANDS라는 torchaudio의 기본 데이터셋 클래스를 상속받아 SubsetSC라는 새로운 클래스를 정의\n",
    "# 이 클래스는 데이터셋의 일부분(예: 훈련, 검증, 테스트)을 선택적으로 로드하는 기능을 추가\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None): # 클래스 생성자 (초기화 메서드), subset 매개변수는 어떤 부분집합을 로드할지 지정하는 문자열\n",
    "        super().__init__(\"./data\", download=True) # 부모 클래스 SPEECHCOMMANDS의 초기화 메소드를 호출, 데이터셋을 \"./data\" 디렉터리에 다운로드 및 저장하며, 필요시 자동으로 다운로드하도록 설정\n",
    "        def load_list(filename): # 주어진 파일 이름을 기반으로 파일 목록을 로드하는 내부 함수 \n",
    "            filepath = os.path.join(self._path, filename) # filename과 데이터셋의 기본 경로(self._path)를 결합하여 전체 파일 경로 설정 \n",
    "            if not os.path.exists(filepath): # 지정한 경로에 파일이 존재하지 않으면 경고 메시지를 출력하고, 빈 리스트를 반환. 즉, 파일이 없으면 건너 뜀\n",
    "                print(f\"File {filepath} not found. Skipping...\")\n",
    "                return []\n",
    "            with open(filepath) as f: # 파일이 존재하면 해당 파일을 열고, 각 줄을 읽음 \n",
    "                return [os.path.join(self._path, line.strip()) for line in f]\n",
    "        if subset == \"validation\": # subset 매개변수가 \"validation\"이면, validation_list.txt 파일을 로드하여 _walker에 저장\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\": # subset이 \"testing\"인 경우,testing_list.txt를 로드하여 _walker에 저장\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\": # subset이 \"training\"인 경우, 검증(validation)과 테스트(testing) 파일 목록을 로드하여 하나의 리스트로 합침\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\") # 검증 파일 + 평가 파일을 excludes에 저장\n",
    "            excludes = set(excludes) # 이 목록을 set으로 변환하여 중복 제거 및 빠른 조회가 가능하도록 수정\n",
    "            self._walker = [w for w in self._walker if w not in excludes] # excludes에 있는 파일 제외한 파일만 _walker\n",
    "\n",
    "# 요약:\n",
    "# SubsetSC 클래스는 SpeechCommands 데이터셋의 특정 부분집합을 로드할 수 있게 해줍니다.\n",
    "# subset 매개변수에 따라 검증, 테스트 또는 훈련 데이터 파일 목록을 _walker에 설정합니다.\n",
    "# 훈련 데이터셋의 경우, 검증과 테스트 파일 목록을 제외한 나머지 파일들을 선택합니다.\n",
    "\n",
    "\n",
    "\n",
    "# 2. Transformer 기반 음성 명령 인식 모델 정의\n",
    "#   -------------------------------------------------------------------\n",
    "#    input_dim: 입력 특성 차원 (예: Mel-spectrogram의 멜 밴드 수)            - 128차원\n",
    "#    num_classes: 분류할 클래스의 개수.\n",
    "#    d_model: Transformer 내부의 임베딩 차원.                              - 128 차원 \n",
    "#    nhead: 멀티헤드 어텐션에서의 헤드 수.                                  - 8개 \n",
    "#    num_layers: Transformer 인코더 레이어의 수.                            - 4개\n",
    "#    dim_feedforward: 각 Transformer 레이어 내 피드포워드 네트워크의 차원. - 512차원\n",
    "#    dropout: 드롭아웃 비율.\n",
    "#   -------------------------------------------------------------------\n",
    "class SpeechTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, d_model=128, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1):\n",
    "        super(SpeechTransformer, self).__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, d_model) # 입력 특성 차원(input_dim)을 Transformer 모델에서 사용하는 차원(d_model)으로 선형 변환하는 레이어\n",
    "        # 단일 Transformer 인코더 레이어를 생성. encoder_layer는 init 메서드 외에서는 사용되지 않으므로 지역 변수로 생성함\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        # 단일 Transformer 인코더를 num_layers만큼 쌓아서 \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        #print (x)\n",
    "        out = self.input_projection(x)             # -> (batch_size, seq_len, d_model)\n",
    "        #print(out)\n",
    "        out = out.transpose(0, 1)                    # Transformer 입력 shape: (seq_len, batch_size, d_model)\n",
    "        out = self.transformer_encoder(out)          # -> (seq_len, batch_size, d_model)\n",
    "        #print(out)\n",
    "        out = out.mean(dim=0)                        # 시퀀스 차원 평균 -> (batch_size, d_model)\n",
    "        #print(out)\n",
    "        logits = self.classifier(out)              # -> (batch_size, num_classes)\n",
    "        #print(logits)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. 평가 함수 정의\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 4. main 함수 정의\n",
    "def main():\n",
    "    transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    train_set = SubsetSC(\"training\")\n",
    "    test_set = SubsetSC(\"testing\")\n",
    "\n",
    "    # 라벨 목록 및 인덱스 매핑 생성\n",
    "    labels = sorted(list(set(dat[2] for dat in train_set)))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        waveforms = []\n",
    "        targets = []\n",
    "        for waveform, sample_rate, label, *_ in batch:\n",
    "            mel_spec = transform(waveform)\n",
    "            mel_spec = mel_spec.mean(dim=0)\n",
    "            mel_spec = mel_spec.transpose(0, 1)\n",
    "            waveforms.append(mel_spec)\n",
    "            targets.append(label_to_idx[label])\n",
    "        waveforms = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)\n",
    "        targets = torch.tensor(targets)\n",
    "        return waveforms, targets\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    input_dim = 128\n",
    "    num_classes = len(labels)\n",
    "    model = SpeechTransformer(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
